# -*- coding: utf-8 -*-
"""CustomerChurning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hQMZrlsjC9Ee7ohCYYL42Vai0JIl72PS

**Customer churn is a major problem and one of the most important concerns for large companies. Due to the direct effect on the revenues of the companies, especially in the telecom field, companies are seeking to develop means to predict potential customer churn. Therefore, finding factors that increase customer churn is important to take necessary actions to reduce this churn. The main contribution of this work is to develop a churn prediction model that assists telecom operators in predicting customers who are most likely subject to churn.**

Loading Necessary Libraries and Mounting Drive
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
import pickle
import sklearn.metrics

"""Loading the Dataset as "churn"
"""

churn = pd.read_csv('/content/drive/My Drive/CustomerChurning/CustomerChurn_dataset.csv')
og = churn.copy()

"""Removing "Customer_ID" from the dataset"""

churn.drop("customerID",axis =1, inplace = True)
og.drop("customerID", axis=1, inplace= True)

churn.info()

churn.head()

"""FEATURE ENGINEERING
-IMPUTING
"""

# Imputing Numerical Columns
numerical_imputer = SimpleImputer(strategy='mean')
for column in churn.select_dtypes(include= "number").columns:
  churn[column] = numerical_imputer.fit_transform(churn.select_dtypes(include='number'))

#Imputing Categorical Columns
for column in churn.select_dtypes(include= "object").columns:
  churn[column] = churn[column].fillna(churn[column].mode()[0])

"""FEATURE ENGINEERING: ENCODING"""

# Label encode categorical variables
label_encoder = LabelEncoder()

for column in churn.select_dtypes(include='object').columns:
    churn[column] = label_encoder.fit_transform(churn[column])

"""
Question 1: Using the given dataset, extract the relevant features that can define a customer churn."""

X = churn.drop(columns=["Churn"])
Y = churn["Churn"]

# Scale the features
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
new_X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)
scaled_Y = scaler.fit_transform(pd.DataFrame(Y))
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(new_X, scaled_Y, test_size=0.2, random_state=42)

# RandomForestRegressor
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]
#putting important features in a list for Question 3.
features = []
for i in range(X.shape[1]):
  features.append(X.columns[indices[i]] )

# Plotting the feature importances
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
plt.bar(range(X.shape[1]), importances[indices], align="center", color= "purple")
plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)
plt.xlabel("Feature")
plt.ylabel("Importance")
plt.title("Relevant Features of CustomerChurn as per RandomForestRegressor")
plt.show()

"""Question 2:

Use your EDA(Exploratory Data Analysis) skills to find out which customer profiles relate to churning a lot.

"""

#Installing necessary libraries
!pip install -U dataprep

import dataprep.eda as dp

#Plotting the correlation between churn and all other variables in the dataset
dp.plot_correlation(churn, "Churn")

more_churn = []
#Plotting bivariate displays of variables in the dataset against churn
for column in churn.columns:
  if column != "Churn":
    display(dp.plot(churn, column, "Churn" ))
    #Identify groups more likely to churn
    mean_churn = churn.groupby(column)["Churn"].mean()
    max_mean_group = mean_churn.idxmax()
    #Save profiles for the group with the maximum mean churn
    more_churn.append({'column': column,'group_id': max_mean_group,})

more_churn

"""more_churn is a list of column names and the groups they contain that are more likely to churn. By comparing the labels of the original dataset above, to the current imputed and encoded one. We can make sense of the data in more_churn"""

churn.head(12)

#Original dataset
og.head(12)

"""Profiles more likely to churn are: Females without partners and dependents, that have phoneservice, multiple phone lines and fibre optic internet service, but no onlinesecurity, online backup, device protection, techsupport, streamingTV, streamingMovies. They also have month-to-month contracts, use paperless billing and pay by electronic check.

QUESTION 3:Using the features in (1) define and train a Multi-Layer Perceptron model using the Functional API
"""

features

pip install scikeras

"""
Creating and Training the Model and recording it's accuracy
"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import GridSearchCV
from keras.models import Model
from keras.layers import Input, Dense
from scikeras.wrappers import KerasClassifier
from sklearn.preprocessing import StandardScaler

#Scaling the data
scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(churn[features]), columns=features)
Y = scaler.fit_transform(pd.DataFrame(churn["Churn"]))

#Labelling encoding so we can use KerasClassifier
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(Y)


# Split the data into training and testing sets
X_Train, X_Test, y_Train, y_Test = train_test_split(X,y_encoded, test_size=0.2, random_state=42)

# Define the Keras functional API model
def create_model(optimizer='adam', hidden_units=64):
    inputs = Input(shape=(19,))
    hidden = Dense(64, activation='relu')(inputs)
    outputs = Dense(1, activation='sigmoid')(hidden)
    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Wrap the Keras model with the scikit-learn interface
keras_model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)


# Train and evaluate the model
keras_model.fit(X_Train, y_Train)
y_pred = keras_model.predict(X_Test)
auc_score = roc_auc_score(y_Test, y_pred)
print(f'AUC Score before GridSearchCV: {auc_score}')

"""Optimizing, CrossValidating and Retraining the model using GridSearchCV"""

# Define the hyperparameters to search
param_grid = {
    'optimizer': ['adam', 'sgd'],
    "epochs" : [10, 20, 30],
    "batch_size": [32, 64, 100]
    }

# Create a GridSearchCV object
grid_search = GridSearchCV(estimator=keras_model, param_grid=param_grid, scoring='roc_auc', cv=3)
grid_result = grid_search.fit(X_Train, y_Train)

# Get the best parameters and their corresponding AUC score
best_params = grid_result.best_params_
best_auc = grid_result.best_score_
print(f'Best Parameters: {best_params}')
print(f'Best AUC Score: {best_auc}')

# Train and evaluate the model with the best parameters
best_model = grid_result.best_estimator_
best_model.fit(X_Train, y_Train)

y_pred_best = best_model.predict(X_Test)
best_auc_score = roc_auc_score(y_Test, y_pred_best)
best_auc_score

"""Saving the Model for Deployment later"""

import pickle
pickle.dump(best_model, open('modell.pkl','wb'))
pickle.dump(scaler, open("scale.pkl", "wb"))